# DL_Theory 
Reposit√≥rio de notebooks did√°ticos cobrindo a teoria e a pr√°tica de **Deep Learning**, desde os conceitos mais fundamentais at√© arquiteturas avan√ßadas como Transformers e Mecanismos de Aten√ß√£o.
---
## Conte√∫do
Os notebooks est√£o organizados em ordem crescente de complexidade:
### üîπ Fundamentos e Pr√©-processamento
- Introdu√ß√£o ao Deep Learning
- Pr√©-processamento de dados para redes neurais
  - Normaliza√ß√£o e padroniza√ß√£o
  - Tratamento de dados faltantes
  - Encoding de vari√°veis categ√≥ricas
  - Divis√£o de conjuntos (treino / valida√ß√£o / teste)
### üîπ Redes Neurais B√°sicas
- **MLP (Multi-Layer Perceptron)**: Arquitetura, fun√ß√£o de ativa√ß√£o, backpropagation e gradiente descendente
- Fun√ß√µes de perda e m√©tricas de avalia√ß√£o
- Regulariza√ß√£o: Dropout, L1 e L2
### üîπ Redes Convolucionais (CNNs)
- Opera√ß√£o de convolu√ß√£o e pooling
- Arquiteturas cl√°ssicas (LeNet, VGG, ResNet)
- Transfer Learning e Fine-tuning
### üîπ Redes Recorrentes (RNNs)
- RNN simples e o problema do gradiente que desaparece
- **LSTM** (Long Short-Term Memory)
- **GRU** (Gated Recurrent Unit)
### üîπ Mecanismo de Aten√ß√£o (*Attention*)
- Aten√ß√£o de Bahdanau
- Self-Attention e Multi-Head Attention
- Positional Encoding
### üîπ Transformers
- Arquitetura Encoder-Decoder do Transformer original (*"Attention is All You Need"*)
- Modelos pr√©-treinados: BERT, GPT e variantes
- Aplica√ß√µes em NLP e Vis√£o Computacional (Vision Transformer ‚Äì ViT)
---
Este reposit√≥rio tem como objetivo servir de material de estudo e refer√™ncia para quem deseja aprender Deep Learning de forma gradual e pr√°tica, com exemplos comentados e exerc√≠cios ao longo dos notebooks.
---
